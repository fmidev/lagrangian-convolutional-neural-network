# unique identifier for metrics calculation experiment
exp_id: "lcnn-logcosh-21082022"

config_copy_path: "results/{id}/{id}_config.yaml"

# the path to the CSV file recording which metrics have been calculated
done_csv_path: "results/{id}/done_predict_{id}.csv"

# the path to the file containing logging output for the experiment
logging_path: "results/{id}/{id}.log"

# template path to the npy files recording the final values of metrics that have been calculated
metrics_npy_path: "results/{id}/{method}/{id}_{metric}_{method}.npy"

# path to the text file containing metric names in order
name_path: "results/{id}/{method}/{id}_{metric}_{method}_names.txt"

# template path for the npy file storing contigency tables recording partial metric calculations
tables_path: "results/{id}/{method}/table_{method}.npy"

# the path to the text file containing a list of timestamps to calculate metrics on
timestamps_path: "/fmi/scratch/project_2005001/LCNN/datelists/fmi_rainy_days_bbox_predict.txt"

# which prediction method name to calculate the metrics on,
# path to the predictions, and number of samples used for predictions calculation input starting
# starting at the current timestamp
methods:
  # L-CNN-d RMSE
  lcnn-diff-rmse-30lt-20062022:
    path: /fmi/scratch/project_2005001/nowcasts/lcnn/lcnn_diff_rmse_30lt_20062022/lcnn_diff_rmse_30lt_20062022_36.h5
  # Rainnet logcosh
  t11-rn-logcosh-lt30:
    path: /fmi/scratch/project_2005001/nowcasts/rainnet-pytorch/p15-rn-logcosh-lt30.hdf5
  # extrapolation
  extrapolation:
    path: /fmi/scratch/project_2005001/nowcasts/pysteps/p25_extrapolation_lcnn_test_swap.hdf5
  # LINDA domain
  linda:
    path: /fmi/scratch/project_2005001/nowcasts/pysteps/p25_linda_lcnn_test_swap.hdf5


# measurement path
measurements:
  path: "/fmi/scratch/project_2005001/nowcasts/measurements/test_obs_512.hdf5"

# leadtimes to calculate the metrics for as units of 5 minutes
# SHOULD NOT BE CHANGED BETWEEN RUNS TO CONTINUE SAVING TO THE SAME .NPY FILE
n_leadtimes: 36

verbose: false
# debugging flag, so that we don't try and go trough all the samples, but only 10
debugging: False
# If set to false, existing contingency tables will simply be used to compute metric values
accumulate: True
# number of chunks to divide the timestamps in for parallelization
# for NO parallelization set to 0
# otherwise set to a positive integer smalller than the number of test samples
# recommended is to set equal or bigger to the number of available processing units
n_chunks: 100
n_workers: 20

# if set to True, will mask all predictions the same, using logical and operation
common_mask: True

# which metrics to calculate
metrics: ["FSS", "CAT", "CONT", "RAPSD", "INTENSITY_SCALE", "SSIM"]

# optional metric-wise parameters
metric_params:
  # Continuous metrics to compute
  cont_metrics: ["MAE", "ME"]
  # categorical metrics to compute
  cat_metrics: ["POD", "FAR", "CSI", "ETS"]
  # thresholds for categorical, spatial metrics
  thresh: [0.5, 1.0, 2.5, 5.0, 10.0, 20.0, 30.0]
  # scales for spatial metrics
  scales: [1, 2, 4, 8, 16]
  # RAPSD parameters
  rapsd:
    # leadtimes for which to calculate RAPSD
    leadtimes: [1, 3, 6, 12, 18, 24, 30, 36]
    # input prediction size
    im_size: [512, 512]
  ssim:
    win_size: 11
